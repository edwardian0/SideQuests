{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9a8c9dc",
   "metadata": {},
   "source": [
    "# MPNN Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7017bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.rdmolops import GetAdjacencyMatrix\n",
    "from preprocessing.featurisation import get_atom_features, get_bond_features\n",
    "from preprocessing.fetch_smiles import resolve_smiles_by_cas_interactive\n",
    "from preprocessing.smiles_to_graph import batch_from_csv\n",
    "\n",
    "from models.mpnn_model import MPNNModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b301934",
   "metadata": {},
   "source": [
    "## Importing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59d4c913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Data(x=[8, 79], edge_index=[2, 16], edge_attr=[16, 10], y=[1]),\n",
       " Data(x=[11, 79], edge_index=[2, 24], edge_attr=[24, 10], y=[1]),\n",
       " Data(x=[10, 79], edge_index=[2, 22], edge_attr=[22, 10], y=[1]),\n",
       " Data(x=[5, 79], edge_index=[2, 10], edge_attr=[10, 10], y=[1]),\n",
       " Data(x=[10, 79], edge_index=[2, 18], edge_attr=[18, 10], y=[1])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_list = batch_from_csv(\"data/processed/input.csv\")\n",
    "graph_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "727615ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of node features: 79\n",
      "Average degree of the graph: 4.00\n",
      "Number of edge features: 10\n"
     ]
    }
   ],
   "source": [
    "num_node_features = graph_list[0].num_node_features\n",
    "num_edge_features = graph_list[0].num_edge_features\n",
    "avg_degree = 2*(graph_list[0].num_edges) / graph_list[0].num_nodes\n",
    "print(f\"Number of node features: {num_node_features}\")\n",
    "print(f\"Average degree of the graph: {avg_degree:.2f}\") # From the avg. degree we can use at least 4 GCNConv layers\n",
    "print(f\"Number of edge features: {num_edge_features}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b47de94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "batch = DataLoader(graph_list, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0157c775",
   "metadata": {},
   "source": [
    "#### Set Random Seed for Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed0ec11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c90b0b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import MSELoss\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "def train_mpnn_model(dataloader, model, lr=1e-3, epochs=300):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = MSELoss()\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            out = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch)\n",
    "            loss = loss_fn(out.squeeze(), batch.y.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "train_loader = DataLoader(graph_list, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ae58bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "def plot_predictions(model, loader):\n",
    "    \"\"\"\n",
    "    Function to plot predictions vs actual values without scaling.\n",
    "    \"\"\"\n",
    "    all_preds, all_targets = [], []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            preds = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch)\n",
    "            all_preds.append(preds.squeeze())\n",
    "            all_targets.append(batch.y.squeeze())\n",
    "\n",
    "    all_preds = torch.cat(all_preds).cpu().numpy()\n",
    "    all_targets = torch.cat(all_targets).cpu().numpy()\n",
    "\n",
    "    r2 = r2_score(all_targets, all_preds)\n",
    "    rmse = np.sqrt(mean_squared_error(all_targets, all_preds))\n",
    "\n",
    "    print(f\"RÂ² score: {r2:.3f}\")\n",
    "    print(f\"RMSE: {rmse:.3f}\")\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(all_targets, all_preds, alpha=0.7)\n",
    "    plt.plot([all_targets.min(), all_targets.max()], \n",
    "             [all_targets.min(), all_targets.max()], 'r--')\n",
    "    plt.xlabel(\"Actual\")\n",
    "    plt.ylabel(\"Predicted\")\n",
    "    plt.title(\"Predicted vs. Actual\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "075ffb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPNNModel(\n",
      "  (edge_nn1): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=5056, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=5056, out_features=5056, bias=True)\n",
      "  )\n",
      "  (conv1): NNConv(79, 64, aggr=add, nn=Sequential(\n",
      "    (0): Linear(in_features=10, out_features=5056, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=5056, out_features=5056, bias=True)\n",
      "  ))\n",
      "  (ffnn): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MPNNModel(in_channels=num_node_features, edge_dim=num_edge_features, hidden_dim=64, out_dim=1, dropout_rate=0.2)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3deaa972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 107.4863\n",
      "Epoch 2, Loss: 126.3471\n",
      "Epoch 3, Loss: 92.2640\n",
      "Epoch 4, Loss: 84.7127\n",
      "Epoch 5, Loss: 93.4396\n",
      "Epoch 6, Loss: 106.2781\n",
      "Epoch 7, Loss: 85.5798\n",
      "Epoch 8, Loss: 84.8889\n",
      "Epoch 9, Loss: 89.5376\n",
      "Epoch 10, Loss: 84.3240\n",
      "Epoch 11, Loss: 80.9187\n",
      "Epoch 12, Loss: 71.3736\n",
      "Epoch 13, Loss: 77.8855\n",
      "Epoch 14, Loss: 97.8183\n",
      "Epoch 15, Loss: 91.7695\n",
      "Epoch 16, Loss: 85.2427\n",
      "Epoch 17, Loss: 82.3048\n",
      "Epoch 18, Loss: 66.6919\n",
      "Epoch 19, Loss: 58.1515\n",
      "Epoch 20, Loss: 69.1456\n",
      "Epoch 21, Loss: 58.9860\n",
      "Epoch 22, Loss: 47.9104\n",
      "Epoch 23, Loss: 64.5834\n",
      "Epoch 24, Loss: 87.6238\n",
      "Epoch 25, Loss: 42.6985\n",
      "Epoch 26, Loss: 87.0997\n",
      "Epoch 27, Loss: 47.8797\n",
      "Epoch 28, Loss: 77.2109\n",
      "Epoch 29, Loss: 79.9216\n",
      "Epoch 30, Loss: 72.7383\n",
      "Epoch 31, Loss: 87.5580\n",
      "Epoch 32, Loss: 54.6522\n",
      "Epoch 33, Loss: 56.7840\n",
      "Epoch 34, Loss: 62.1017\n",
      "Epoch 35, Loss: 40.4292\n",
      "Epoch 36, Loss: 36.3215\n",
      "Epoch 37, Loss: 64.7751\n",
      "Epoch 38, Loss: 43.8276\n",
      "Epoch 39, Loss: 41.3344\n",
      "Epoch 40, Loss: 48.8835\n",
      "Epoch 41, Loss: 40.6066\n",
      "Epoch 42, Loss: 81.8737\n",
      "Epoch 43, Loss: 47.2752\n",
      "Epoch 44, Loss: 44.7051\n",
      "Epoch 45, Loss: 50.7856\n",
      "Epoch 46, Loss: 45.4088\n",
      "Epoch 47, Loss: 35.3902\n",
      "Epoch 48, Loss: 42.3520\n",
      "Epoch 49, Loss: 38.4748\n",
      "Epoch 50, Loss: 50.0274\n",
      "Epoch 51, Loss: 55.5147\n",
      "Epoch 52, Loss: 35.1585\n",
      "Epoch 53, Loss: 38.7397\n",
      "Epoch 54, Loss: 53.0395\n",
      "Epoch 55, Loss: 34.3399\n",
      "Epoch 56, Loss: 32.1119\n",
      "Epoch 57, Loss: 42.2356\n",
      "Epoch 58, Loss: 42.6823\n",
      "Epoch 59, Loss: 38.9716\n",
      "Epoch 60, Loss: 34.1701\n",
      "Epoch 61, Loss: 30.7356\n",
      "Epoch 62, Loss: 41.2639\n",
      "Epoch 63, Loss: 61.7031\n",
      "Epoch 64, Loss: 32.0586\n",
      "Epoch 65, Loss: 44.0822\n",
      "Epoch 66, Loss: 34.4781\n",
      "Epoch 67, Loss: 43.0162\n",
      "Epoch 68, Loss: 35.1184\n",
      "Epoch 69, Loss: 54.9893\n",
      "Epoch 70, Loss: 34.8576\n",
      "Epoch 71, Loss: 25.4830\n",
      "Epoch 72, Loss: 31.3800\n",
      "Epoch 73, Loss: 34.6825\n",
      "Epoch 74, Loss: 33.6827\n",
      "Epoch 75, Loss: 29.7771\n",
      "Epoch 76, Loss: 33.6769\n",
      "Epoch 77, Loss: 37.0400\n",
      "Epoch 78, Loss: 42.6621\n",
      "Epoch 79, Loss: 28.8193\n",
      "Epoch 80, Loss: 29.0236\n",
      "Epoch 81, Loss: 29.3813\n",
      "Epoch 82, Loss: 32.1154\n",
      "Epoch 83, Loss: 26.3076\n",
      "Epoch 84, Loss: 57.4503\n",
      "Epoch 85, Loss: 29.7561\n",
      "Epoch 86, Loss: 29.9296\n",
      "Epoch 87, Loss: 26.4890\n",
      "Epoch 88, Loss: 32.0899\n",
      "Epoch 89, Loss: 23.9219\n",
      "Epoch 90, Loss: 40.0686\n",
      "Epoch 91, Loss: 31.5675\n",
      "Epoch 92, Loss: 26.9881\n",
      "Epoch 93, Loss: 34.4494\n",
      "Epoch 94, Loss: 27.7603\n",
      "Epoch 95, Loss: 22.0200\n",
      "Epoch 96, Loss: 25.5862\n",
      "Epoch 97, Loss: 29.4065\n",
      "Epoch 98, Loss: 30.0168\n",
      "Epoch 99, Loss: 25.4479\n",
      "Epoch 100, Loss: 33.2303\n",
      "Epoch 101, Loss: 27.7116\n",
      "Epoch 102, Loss: 31.7745\n",
      "Epoch 103, Loss: 21.8172\n",
      "Epoch 104, Loss: 35.7326\n",
      "Epoch 105, Loss: 33.1847\n",
      "Epoch 106, Loss: 27.7196\n",
      "Epoch 107, Loss: 39.7009\n",
      "Epoch 108, Loss: 23.5850\n",
      "Epoch 109, Loss: 31.7691\n",
      "Epoch 110, Loss: 30.6570\n",
      "Epoch 111, Loss: 20.5795\n",
      "Epoch 112, Loss: 21.5427\n",
      "Epoch 113, Loss: 27.5658\n",
      "Epoch 114, Loss: 22.0242\n",
      "Epoch 115, Loss: 21.4855\n",
      "Epoch 116, Loss: 23.0688\n",
      "Epoch 117, Loss: 30.3121\n",
      "Epoch 118, Loss: 22.2133\n",
      "Epoch 119, Loss: 27.2158\n",
      "Epoch 120, Loss: 26.8563\n",
      "Epoch 121, Loss: 22.2754\n",
      "Epoch 122, Loss: 31.5398\n",
      "Epoch 123, Loss: 24.4538\n",
      "Epoch 124, Loss: 21.1948\n",
      "Epoch 125, Loss: 19.9580\n",
      "Epoch 126, Loss: 25.9776\n",
      "Epoch 127, Loss: 20.0811\n",
      "Epoch 128, Loss: 19.1542\n",
      "Epoch 129, Loss: 16.3584\n",
      "Epoch 130, Loss: 24.5285\n",
      "Epoch 131, Loss: 17.5464\n",
      "Epoch 132, Loss: 21.9481\n",
      "Epoch 133, Loss: 23.7860\n",
      "Epoch 134, Loss: 21.7523\n",
      "Epoch 135, Loss: 18.9454\n",
      "Epoch 136, Loss: 19.6422\n",
      "Epoch 137, Loss: 17.6022\n",
      "Epoch 138, Loss: 18.2087\n",
      "Epoch 139, Loss: 26.9216\n",
      "Epoch 140, Loss: 27.3339\n",
      "Epoch 141, Loss: 27.9323\n",
      "Epoch 142, Loss: 20.5210\n",
      "Epoch 143, Loss: 21.8784\n",
      "Epoch 144, Loss: 18.2584\n",
      "Epoch 145, Loss: 20.5191\n",
      "Epoch 146, Loss: 16.6294\n",
      "Epoch 147, Loss: 13.8668\n",
      "Epoch 148, Loss: 26.1671\n",
      "Epoch 149, Loss: 24.2621\n",
      "Epoch 150, Loss: 18.9587\n",
      "Epoch 151, Loss: 24.9057\n",
      "Epoch 152, Loss: 16.8705\n",
      "Epoch 153, Loss: 27.0329\n",
      "Epoch 154, Loss: 13.8709\n",
      "Epoch 155, Loss: 18.4945\n",
      "Epoch 156, Loss: 18.7521\n",
      "Epoch 157, Loss: 22.8904\n",
      "Epoch 158, Loss: 17.7637\n",
      "Epoch 159, Loss: 18.6643\n",
      "Epoch 160, Loss: 21.2637\n",
      "Epoch 161, Loss: 20.8851\n",
      "Epoch 162, Loss: 22.4073\n",
      "Epoch 163, Loss: 17.6641\n",
      "Epoch 164, Loss: 19.8191\n",
      "Epoch 165, Loss: 15.4346\n",
      "Epoch 166, Loss: 19.3294\n",
      "Epoch 167, Loss: 14.9057\n",
      "Epoch 168, Loss: 18.5103\n",
      "Epoch 169, Loss: 18.1977\n",
      "Epoch 170, Loss: 25.7964\n",
      "Epoch 171, Loss: 23.4681\n",
      "Epoch 172, Loss: 14.1530\n",
      "Epoch 173, Loss: 14.6686\n",
      "Epoch 174, Loss: 14.6352\n",
      "Epoch 175, Loss: 16.7101\n",
      "Epoch 176, Loss: 31.4673\n",
      "Epoch 177, Loss: 14.2358\n",
      "Epoch 178, Loss: 22.9173\n",
      "Epoch 179, Loss: 18.7783\n",
      "Epoch 180, Loss: 16.8799\n",
      "Epoch 181, Loss: 21.5333\n",
      "Epoch 182, Loss: 29.6217\n",
      "Epoch 183, Loss: 27.6049\n",
      "Epoch 184, Loss: 18.0606\n",
      "Epoch 185, Loss: 19.0195\n",
      "Epoch 186, Loss: 27.7772\n",
      "Epoch 187, Loss: 17.7801\n",
      "Epoch 188, Loss: 14.7491\n",
      "Epoch 189, Loss: 26.5722\n",
      "Epoch 190, Loss: 31.2650\n",
      "Epoch 191, Loss: 14.0078\n",
      "Epoch 192, Loss: 22.1091\n",
      "Epoch 193, Loss: 19.9854\n",
      "Epoch 194, Loss: 16.5527\n",
      "Epoch 195, Loss: 14.3357\n",
      "Epoch 196, Loss: 14.9629\n",
      "Epoch 197, Loss: 17.6171\n",
      "Epoch 198, Loss: 18.0351\n",
      "Epoch 199, Loss: 36.3374\n",
      "Epoch 200, Loss: 12.9226\n",
      "Epoch 201, Loss: 22.3473\n",
      "Epoch 202, Loss: 26.7811\n",
      "Epoch 203, Loss: 22.5288\n",
      "Epoch 204, Loss: 17.5568\n",
      "Epoch 205, Loss: 22.5264\n",
      "Epoch 206, Loss: 21.3200\n",
      "Epoch 207, Loss: 14.7394\n",
      "Epoch 208, Loss: 19.3639\n",
      "Epoch 209, Loss: 15.9782\n",
      "Epoch 210, Loss: 16.6935\n",
      "Epoch 211, Loss: 22.2248\n",
      "Epoch 212, Loss: 13.8946\n",
      "Epoch 213, Loss: 16.7622\n",
      "Epoch 214, Loss: 16.0507\n",
      "Epoch 215, Loss: 14.3785\n",
      "Epoch 216, Loss: 17.5766\n",
      "Epoch 217, Loss: 21.3240\n",
      "Epoch 218, Loss: 17.1336\n",
      "Epoch 219, Loss: 11.3721\n",
      "Epoch 220, Loss: 16.0983\n",
      "Epoch 221, Loss: 18.1089\n",
      "Epoch 222, Loss: 20.8561\n",
      "Epoch 223, Loss: 16.8658\n",
      "Epoch 224, Loss: 19.9909\n",
      "Epoch 225, Loss: 16.2063\n",
      "Epoch 226, Loss: 22.5433\n",
      "Epoch 227, Loss: 13.6799\n",
      "Epoch 228, Loss: 19.1744\n",
      "Epoch 229, Loss: 12.9739\n",
      "Epoch 230, Loss: 17.9243\n",
      "Epoch 231, Loss: 31.8101\n",
      "Epoch 232, Loss: 19.0588\n",
      "Epoch 233, Loss: 26.0065\n",
      "Epoch 234, Loss: 17.5503\n",
      "Epoch 235, Loss: 16.4532\n",
      "Epoch 236, Loss: 18.2950\n",
      "Epoch 237, Loss: 22.2780\n",
      "Epoch 238, Loss: 12.6731\n",
      "Epoch 239, Loss: 24.9035\n",
      "Epoch 240, Loss: 21.1119\n",
      "Epoch 241, Loss: 20.3322\n",
      "Epoch 242, Loss: 16.4743\n",
      "Epoch 243, Loss: 16.0548\n",
      "Epoch 244, Loss: 15.7205\n",
      "Epoch 245, Loss: 15.9761\n",
      "Epoch 246, Loss: 21.6850\n",
      "Epoch 247, Loss: 20.8672\n",
      "Epoch 248, Loss: 12.3940\n",
      "Epoch 249, Loss: 18.4292\n",
      "Epoch 250, Loss: 16.2376\n",
      "Epoch 251, Loss: 13.9502\n",
      "Epoch 252, Loss: 12.9689\n",
      "Epoch 253, Loss: 23.8555\n",
      "Epoch 254, Loss: 16.8962\n",
      "Epoch 255, Loss: 14.4599\n",
      "Epoch 256, Loss: 17.1336\n",
      "Epoch 257, Loss: 13.8821\n",
      "Epoch 258, Loss: 16.8600\n",
      "Epoch 259, Loss: 23.2119\n",
      "Epoch 260, Loss: 19.7656\n",
      "Epoch 261, Loss: 13.4029\n",
      "Epoch 262, Loss: 22.8669\n",
      "Epoch 263, Loss: 16.5857\n",
      "Epoch 264, Loss: 20.1643\n",
      "Epoch 265, Loss: 16.8624\n",
      "Epoch 266, Loss: 15.9907\n",
      "Epoch 267, Loss: 15.7834\n",
      "Epoch 268, Loss: 12.4427\n",
      "Epoch 269, Loss: 18.6749\n",
      "Epoch 270, Loss: 16.0314\n",
      "Epoch 271, Loss: 13.5652\n",
      "Epoch 272, Loss: 14.3867\n",
      "Epoch 273, Loss: 23.2306\n",
      "Epoch 274, Loss: 13.8398\n",
      "Epoch 275, Loss: 17.9344\n",
      "Epoch 276, Loss: 12.7284\n",
      "Epoch 277, Loss: 12.1440\n",
      "Epoch 278, Loss: 15.1134\n",
      "Epoch 279, Loss: 14.1425\n",
      "Epoch 280, Loss: 13.1058\n",
      "Epoch 281, Loss: 11.8686\n",
      "Epoch 282, Loss: 9.8897\n",
      "Epoch 283, Loss: 14.2649\n",
      "Epoch 284, Loss: 12.2207\n",
      "Epoch 285, Loss: 19.7463\n",
      "Epoch 286, Loss: 16.3564\n",
      "Epoch 287, Loss: 14.5608\n",
      "Epoch 288, Loss: 19.7694\n",
      "Epoch 289, Loss: 17.0598\n",
      "Epoch 290, Loss: 16.9905\n",
      "Epoch 291, Loss: 19.8428\n",
      "Epoch 292, Loss: 14.9506\n",
      "Epoch 293, Loss: 28.1049\n",
      "Epoch 294, Loss: 16.5052\n",
      "Epoch 295, Loss: 14.7811\n",
      "Epoch 296, Loss: 10.5505\n",
      "Epoch 297, Loss: 16.2849\n",
      "Epoch 298, Loss: 13.0554\n",
      "Epoch 299, Loss: 10.7890\n",
      "Epoch 300, Loss: 16.3342\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MPNNModel(\n",
       "  (edge_nn1): Sequential(\n",
       "    (0): Linear(in_features=10, out_features=5056, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=5056, out_features=5056, bias=True)\n",
       "  )\n",
       "  (conv1): NNConv(79, 64, aggr=add, nn=Sequential(\n",
       "    (0): Linear(in_features=10, out_features=5056, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=5056, out_features=5056, bias=True)\n",
       "  ))\n",
       "  (ffnn): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mpnn_model(train_loader, model, lr=1e-3, epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48941b47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
